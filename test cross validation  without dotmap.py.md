
---

## ðŸŒ² Tree Counting and Object Detection Evaluation (Faster R-CNN, Multi-Fold, IoU â‰¥ 0.5)

### ðŸ” Overview

This repository implements a **multi-fold evaluation pipeline** for object detection using Faster R-CNN, tailored for counting trees in high-density forest images **without red-dot annotations**.

The pipeline covers:

* ðŸ“ **Cross-validation (5-fold)**
* ðŸ“¦ Model prediction and CSV export per image
* ðŸ§¼ IoU-based filtering of predictions (â‰¥ 0.5)
* ðŸ“Š Confusion matrix & metrics (precision, recall, F1-score)
* ðŸ“ˆ Aggregated final Excel report across folds
* ðŸŒ² Per-fold tree count from `merged_predictions.csv`

---

### ðŸ“ Directory Structure

```
E:\FASTRCNN\FASTRCNN\dataset2\balanced_dataset\
â”‚
â””â”€â”€ checkpoints_cross-without dotmap
    â”œâ”€â”€ fold{0..4}_best.pth        â† Checkpoints
    â”œâ”€â”€ result{0..4}\
    â”‚   â””â”€â”€ COMBINE_RESULT\
    â”‚       â”œâ”€â”€ predicted_boxes\   â† Raw predictions per image
    â”‚       â”œâ”€â”€ filtered_predicted_dots\ â† After IoU filtering
    â”‚       â”œâ”€â”€ merged_predictions.csv
    â”‚       â”œâ”€â”€ confusion_matrix.csv
    â”‚       â”œâ”€â”€ metrics.xlsx
    â”‚       â”œâ”€â”€ total_samples.csv
    â”‚       â””â”€â”€ result_matrix.csv
    â”‚
    â”œâ”€â”€ final_report.xlsx          â† Multi-sheet summary (all folds)
    â””â”€â”€ tree_count_report.csv      â† Total rows per merged_predictions.csv
```

---

### âš™ï¸ Key Functionalities

#### âœ… **1. Model Inference (Faster R-CNN)**

Each image in the test set is passed through a pretrained and fine-tuned Faster R-CNN model to generate bounding box predictions.

#### âœ… **2. IoU Filtering (â‰¥ 0.5)**

Predictions are filtered using IoU with the best-matching ground-truth box per image:

```python
if iou â‰¥ 0.5 and higher than previous best â†’ accept
```

#### âœ… **3. Metrics Computation**

* Confusion matrix (with class 0 zeroed)
* Per-class precision, recall, F1-score
* Macro average and accuracy
* Stored in both `.csv` and `.xlsx`

#### âœ… **4. Aggregated Reports**

A final Excel workbook is created with:

* `Metrics`: per-fold + average
* `Total Trees`: number of predicted rows (after filtering)
* `Confusion Matrix`: raw matrices
* `Total Samples`: total predictions from confusion matrix
* `Tree Count`: from `merged_predictions.csv` line count

---

### ðŸ“„ Output Reports

| File                     | Description                                   |
| ------------------------ | --------------------------------------------- |
| `metrics.xlsx`           | Per-fold evaluation metrics                   |
| `total_samples.csv`      | Number of predictions (from confusion matrix) |
| `merged_predictions.csv` | Final filtered predictions per fold           |
| `confusion_matrix.csv`   | Confusion matrix used for metrics             |
| `tree_count_report.csv`  | Number of prediction rows per fold            |
| `final_report.xlsx`      | Consolidated report across all folds          |

---

### ðŸ“Š Example Metrics (Macro Avg)

| Metric    | Value  |
| --------- | ------ |
| Precision | 0.8421 |
| Recall    | 0.7893 |
| F1-Score  | 0.8125 |
| Accuracy  | 0.8604 |

> ðŸ“Œ These are **placeholders**. Actual values are computed and stored in `final_report.xlsx`.

---

### ðŸ“¦ Dependencies

* Python 3.8+
* PyTorch â‰¥ 1.13
* torchvision
* pandas, numpy, matplotlib
* openpyxl, xlsxwriter
* tqdm
* seaborn (optional)

---

### ðŸš€ Run Instructions

1. Place your trained checkpoints as:
   `checkpoints_cross-without dotmap/fold0_best.pth` ... `fold4_best.pth`

2. Run the script directly (no CLI args needed):

   ```bash
   python evaluate_multifold.py
   ```

3. Results will be saved to:
   `checkpoints_cross-without dotmap/final_report.xlsx`

---

### ðŸ§  Notes

* The pipeline auto-generates missing directories and handles empty or corrupt CSVs gracefully.
* All `.tif` image names are normalized during merging and filtering.
* Filtering logic ensures **only the best prediction per ground-truth box** is kept.

---

### ðŸ“¬ Citation / Acknowledgment

If you use this pipeline or its components, please consider citing or referencing the original code authors or the research context it was built for.

