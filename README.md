


## 📝 How to Use This Repository

This project contains a complete pipeline for training and evaluating a **tree counting model** using deep learning and cross-validation, both with and without red-dot supervision.

### 📥 1. Download Preprocessed Balanced Dataset and Checkpoints

First, download the preprocessed, **balanced dataset** containing tree bounding boxes generated by [TreeCountNet](https://github.com/your/treecountnet) along with all required assets:

🔗 **Dataset & Resources:**
[https://yun.ir/tqxdd1](https://yun.ir/tqxdd1)

> The downloaded content includes:
>
> * Balanced dataset
> * Label files for cross-validation
> * Folder structure for prediction outputs
> * Red point annotations (if applicable)

---

### 🏋️‍♂️ 2. Train Model Using Cross-Validation

Run the training script to train Faster R-CNN models across 5 folds:

```bash
python 02_train_on_tree_3_crossvalidation.py
```

> ⚙️ Make sure to set the correct path to the dataset in the script before running.

This will generate five checkpoints:

```
checkpoints_cross-without dotmap/
├── fold0_best.pth
├── fold1_best.pth
...
├── fold4_best.pth
```

---

### 🧪 3. Evaluate Model Without Red-Point Coordinates

To simulate **real-world performance** without red-dot supervision, use the following evaluation script:

```bash
python test_crossvalidation_without_dotmap.py
```

This script will:

* Predict bounding boxes on test images
* Filter predictions using IoU ≥ 0.5
* Generate confusion matrices, metrics, and merged results
* Save all results in:

  ```
  checkpoints_cross-without dotmap/result{0..4}/COMBINE_RESULT/
  ```
If the number of predicted trees in the script
test cross validation without dotmap.py
is not correct after execution, please run the following script with the correct path:
Aggregate the number of trees.py
---

### 📈 4. Evaluate With Red-Point Ground Truth (Paper Experiments)

For paper-level evaluations and comparisons using red-point annotations, run:

```bash
python test_crossvalidation_with_redpoint.py
```

This script performs a detailed 5-fold evaluation using red-dot ground-truth annotations and generates:

* CSVs for:

  * Confusion matrices
  * Evaluation metrics
  * Total number of predicted trees per fold (`total_samples.csv`)
  * Number of rows in `merged_predictions.csv` per fold (`tree_count_report.csv`)
* A multi-sheet Excel report: `final_report.xlsx`

These results support deeper analysis and visualizations used in the article.

---

### 📊 Output Summary

| Output File / Sheet     | Description                                      |
| ----------------------- | ------------------------------------------------ |
| `metrics.xlsx`          | Precision, recall, F1-score per class            |
| `total_samples.csv`     | Total predictions after IoU filtering            |
| `tree_count_report.csv` | Number of predicted boxes per fold               |
| `confusion_matrix.csv`  | Raw confusion matrix per fold                    |
| `final_report.xlsx`     | Aggregated metrics, total counts, confusion data |

---

### 🧠 Notes

* IoU threshold for filtering predictions is set to `0.5`.
* All results are saved in per-fold folders for better organization.
* Excel reports use color formatting for better readability.

---

